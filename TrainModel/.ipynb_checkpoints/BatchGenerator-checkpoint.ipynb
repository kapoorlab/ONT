{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indoor-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append(\"../\")\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from NEATUtils import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "returning-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of  images:\t 1689\n",
      "image size:\t\t (1689, 128, 128, 1)\n",
      "Labels:\t\t\t\t (1689, 1, 1, 9)\n"
     ]
    }
   ],
   "source": [
    "ImageLabelDir = '/home/sancere/Kepler/FinalONEATTraining/StaticCenterTrainData/'\n",
    "categories = 4\n",
    "anchors = [0.11,0.11, 0.17,0.19, 0.26,0.29, 0.37,0.37, 0.62,0.55]\n",
    "nbxes = len(anchors)//2\n",
    "(X,Y), (X_val,Y_val) = helpers.load_full_training_data(ImageLabelDir, categories, nboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class ImageReader(object):\n",
    "    def __init__(self,IMAGE_H,IMAGE_W, norm=None):\n",
    "        '''\n",
    "        IMAGE_H : the height of the rescaled image, e.g., 416\n",
    "        IMAGE_W : the width of the rescaled image, e.g., 416\n",
    "        '''\n",
    "        self.IMAGE_H = IMAGE_H\n",
    "        self.IMAGE_W = IMAGE_W\n",
    "        self.norm    = norm\n",
    "        \n",
    "    def encode_core(self,image, reorder_rgb=True):     \n",
    "        # resize the image to standard size\n",
    "        image = cv2.resize(image, (self.IMAGE_H, self.IMAGE_W))\n",
    "        if reorder_rgb:\n",
    "            image = image[:,:,::-1]\n",
    "        if self.norm is not None:\n",
    "            image = self.norm(image)\n",
    "        return(image)\n",
    "    \n",
    "    def fit(self,train_instance):\n",
    "        '''\n",
    "        read in and resize the image, annotations are resized accordingly.\n",
    "        \n",
    "        -- Input -- \n",
    "        \n",
    "        train_instance : dictionary containing filename, height, width and object\n",
    "        \n",
    "        {'filename': 'ObjectDetectionRCNN/VOCdevkit/VOC2012/JPEGImages/2008_000054.jpg',\n",
    "         'height':   333,\n",
    "         'width':    500,\n",
    "         'object': [{'name': 'bird',\n",
    "                     'xmax': 318,\n",
    "                     'xmin': 284,\n",
    "                     'ymax': 184,\n",
    "                     'ymin': 100},\n",
    "                    {'name': 'bird', \n",
    "                     'xmax': 198, \n",
    "                     'xmin': 112, \n",
    "                     'ymax': 209, \n",
    "                     'ymin': 146}]\n",
    "        }\n",
    "        \n",
    "        '''\n",
    "        if not isinstance(train_instance,dict):\n",
    "            train_instance = {'filename':train_instance}\n",
    "                \n",
    "        image_name = train_instance['filename']\n",
    "        image = cv2.imread(image_name)\n",
    "        h, w, c = image.shape\n",
    "        if image is None: print('Cannot find ', image_name)\n",
    "      \n",
    "        image = self.encode_core(image, reorder_rgb=True)\n",
    "            \n",
    "        if \"object\" in train_instance.keys():\n",
    "            \n",
    "            all_objs = copy.deepcopy(train_instance['object'])     \n",
    "\n",
    "            # fix object's position and size\n",
    "            for obj in all_objs:\n",
    "                for attr in ['xmin', 'xmax']:\n",
    "                    obj[attr] = int(obj[attr] * float(self.IMAGE_W) / w)\n",
    "                    obj[attr] = max(min(obj[attr], self.IMAGE_W), 0)\n",
    "\n",
    "                for attr in ['ymin', 'ymax']:\n",
    "                    obj[attr] = int(obj[attr] * float(self.IMAGE_H) / h)\n",
    "                    obj[attr] = max(min(obj[attr], self.IMAGE_H), 0)\n",
    "        else:\n",
    "            return image\n",
    "        return image, all_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class SimpleBatchGenerator(Sequence):\n",
    "    def __init__(self, images, config, norm=None, shuffle=True):\n",
    "        \n",
    "        self.config = config\n",
    "        self.config[\"BOX\"] = int(len(self.config['ANCHORS'])/2)\n",
    "        self.config[\"CLASS\"] = len(self.config['LABELS'])\n",
    "        self.images = images\n",
    "        self.bestAnchorBoxFinder = BestAnchorBoxFinder(config['ANCHORS'])\n",
    "        self.imageReader = ImageReader(config['IMAGE_H'],config['IMAGE_W'],norm=norm)\n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle: \n",
    "            np.random.shuffle(self.images)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(float(len(self.images))/self.config['BATCH_SIZE']))  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        == input == \n",
    "        \n",
    "        idx : non-negative integer value e.g., 0\n",
    "        \n",
    "        == output ==\n",
    "        \n",
    "        x_batch: The numpy array of shape  (BATCH_SIZE, IMAGE_H, IMAGE_W, N channels).\n",
    "            \n",
    "            x_batch[iframe,:,:,:] contains a iframeth frame of size  (IMAGE_H,IMAGE_W).\n",
    "            \n",
    "        y_batch:\n",
    "\n",
    "            The numpy array of shape  (BATCH_SIZE, GRID_H, GRID_W, BOX, 4 + 1 + N classes). \n",
    "            BOX = The number of anchor boxes.\n",
    "\n",
    "            y_batch[iframe,igrid_h,igrid_w,ianchor,:4] contains (center_x,center_y,center_w,center_h) \n",
    "            of ianchorth anchor at  grid cell=(igrid_h,igrid_w) if the object exists in \n",
    "            this (grid cell, anchor) pair, else they simply contain 0.\n",
    "\n",
    "            y_batch[iframe,igrid_h,igrid_w,ianchor,4] contains 1 if the object exists in this \n",
    "            (grid cell, anchor) pair, else it contains 0.\n",
    "\n",
    "            y_batch[iframe,igrid_h,igrid_w,ianchor,5 + iclass] contains 1 if the iclass^th \n",
    "            class object exists in this (grid cell, anchor) pair, else it contains 0.\n",
    "\n",
    "\n",
    "        b_batch:\n",
    "\n",
    "            The numpy array of shape (BATCH_SIZE, 1, 1, 1, TRUE_BOX_BUFFER, 4).\n",
    "\n",
    "            b_batch[iframe,1,1,1,ibuffer,ianchor,:] contains ibufferth object's \n",
    "            (center_x,center_y,center_w,center_h) in iframeth frame.\n",
    "\n",
    "            If ibuffer > N objects in iframeth frame, then the values are simply 0.\n",
    "\n",
    "            TRUE_BOX_BUFFER has to be some large number, so that the frame with the \n",
    "            biggest number of objects can also record all objects.\n",
    "\n",
    "            The order of the objects do not matter.\n",
    "\n",
    "            This is just a hack to easily calculate loss. \n",
    "        \n",
    "        '''\n",
    "        l_bound = idx*self.config['BATCH_SIZE']\n",
    "        r_bound = (idx+1)*self.config['BATCH_SIZE']\n",
    "\n",
    "        if r_bound > len(self.images):\n",
    "            r_bound = len(self.images)\n",
    "            l_bound = r_bound - self.config['BATCH_SIZE']\n",
    "\n",
    "        instance_count = 0\n",
    "        \n",
    "        ## prepare empty storage space: this will be output\n",
    "        x_batch = np.zeros((r_bound - l_bound, self.config['IMAGE_H'], self.config['IMAGE_W'], 3))                         # input images\n",
    "        b_batch = np.zeros((r_bound - l_bound, 1     , 1     , 1    ,  self.config['TRUE_BOX_BUFFER'], 4))   # list of self.config['TRUE_self.config['BOX']_BUFFER'] GT boxes\n",
    "        y_batch = np.zeros((r_bound - l_bound, self.config['GRID_H'],  self.config['GRID_W'], self.config['BOX'], 4+1+len(self.config['LABELS'])))                # desired network output\n",
    "\n",
    "        for train_instance in self.images[l_bound:r_bound]:\n",
    "            # augment input image and fix object's position and size\n",
    "            img, all_objs = self.imageReader.fit(train_instance)\n",
    "            \n",
    "            # construct output from object's x, y, w, h\n",
    "            true_box_index = 0\n",
    "            \n",
    "            for obj in all_objs:\n",
    "                if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin'] and obj['name'] in self.config['LABELS']:\n",
    "                    center_x, center_y = rescale_centerxy(obj,self.config)\n",
    "                    \n",
    "                    grid_x = int(np.floor(center_x))\n",
    "                    grid_y = int(np.floor(center_y))\n",
    "\n",
    "                    if grid_x < self.config['GRID_W'] and grid_y < self.config['GRID_H']:\n",
    "                        obj_indx  = self.config['LABELS'].index(obj['name'])\n",
    "                        center_w, center_h = rescale_cebterwh(obj,self.config)\n",
    "                        box = [center_x, center_y, center_w, center_h]\n",
    "                        best_anchor,max_iou = self.bestAnchorBoxFinder.find(center_w, center_h)\n",
    "                                \n",
    "                        # assign ground truth x, y, w, h, confidence and class probs to y_batch\n",
    "                        # it could happen that the same grid cell contain 2 similar shape objects\n",
    "                        # as a result the same anchor box is selected as the best anchor box by the multiple objects\n",
    "                        # in such ase, the object is over written\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 0:4] = box # center_x, center_y, w, h\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 4  ] = 1. # ground truth confidence is 1\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 5+obj_indx] = 1 # class probability of the object\n",
    "                        \n",
    "                        # assign the true box to b_batch\n",
    "                        b_batch[instance_count, 0, 0, 0, true_box_index] = box\n",
    "                        \n",
    "                        true_box_index += 1\n",
    "                        true_box_index = true_box_index % self.config['TRUE_BOX_BUFFER']\n",
    "                            \n",
    "            x_batch[instance_count] = img\n",
    "            # increase instance counter in current batch\n",
    "            instance_count += 1  \n",
    "        return [x_batch, b_batch], y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle: \n",
    "            np.random.shuffle(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_H, IMAGE_W = 128, 128\n",
    "BATCH_SIZE       = 16\n",
    "TRUE_BOX_BUFFER  = 50\n",
    "GRID_H = 1\n",
    "GRID_W = 1\n",
    "StaticLabel = [0, 1, 2, 3]\n",
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, \n",
    "    'IMAGE_W'         : IMAGE_W,\n",
    "    'GRID_H'          : GRID_H,  \n",
    "    'GRID_W'          : GRID_W,\n",
    "    'LABELS'          : StaticLabel,\n",
    "    'ANCHORS'         : anchors,\n",
    "    'BATCH_SIZE'      : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : TRUE_BOX_BUFFER,\n",
    "}\n",
    "\n",
    "\n",
    "train_batch_generator = SimpleBatchGenerator(train_image, generator_config,\n",
    "                                             norm=normalize, shuffle=True)\n",
    "\n",
    "[x_batch,b_batch],y_batch = train_batch_generator.__getitem__(idx=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_batch: (BATCH_SIZE, IMAGE_H, IMAGE_W, N channels)           = {}\".format(x_batch.shape))\n",
    "print(\"y_batch: (BATCH_SIZE, GRID_H, GRID_W, BOX, 4 + 1 + N classes) = {}\".format(y_batch.shape))\n",
    "print(\"b_batch: (BATCH_SIZE, 1, 1, 1, TRUE_BOX_BUFFER, 4)            = {}\".format(b_batch.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
